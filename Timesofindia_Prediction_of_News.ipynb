{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Timesofindia_Prediction_of_News.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM3REFr1AG5yBjD3XRH+IkS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vj-Ydv/thesis_work/blob/main/Timesofindia_Prediction_of_News.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LT-O0mUWeR8U",
        "outputId": "40bc8c23-01b8-4bfe-9e20-25529f138a17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (4.64.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.1.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (2022.6.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.21.6)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "# This cell connects to google drive, authenticates connection, and iterates over file list displaying each file's title and ID.\n",
        "# it also imports and installs all the necessary libraries\n",
        "!pip install -U -q PyDrive\n",
        "!pip install -U -q wordcloud\n",
        "!pip install -U -q bokeh\n",
        "!pip install textblob\n",
        "!pip install sklearn\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "from scipy.stats import hmean\n",
        "from scipy.stats import norm\n",
        "from pylab import *\n",
        "from bokeh.plotting import figure\n",
        "from bokeh.io import output_notebook, show\n",
        "from bokeh.models import LinearColorMapper\n",
        "from bokeh.models import HoverTool\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from textblob import TextBlob\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from time import time\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDXE8Oc-edgq",
        "outputId": "90c4275f-8c43-4702-8ea0-fe26be4bec99"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My Drive/Colab Notebooks/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDCZUDhCedj3",
        "outputId": "36ecebc6-bb28-46b1-ec9e-18c0a8bf74eb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-YFUhXzedm7",
        "outputId": "290500db-c9e0-4728-c9fc-6646af4a3a74"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.3.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.7)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.7.8)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.1)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.1.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.8.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.3)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.0.17)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_aspect(text):\n",
        "  # Importing the required libraries\n",
        "  import spacy\n",
        "  sp = spacy.load(\"en_core_web_sm\")\n",
        "  from textblob import TextBlob\n",
        "\n",
        "  # Creating a list of positive and negative sentences.\n",
        "  mixed_sen = [text]\n",
        "\n",
        "  # An empty list for obtaining the extracted aspects\n",
        "  # from sentences.\n",
        "  ext_aspects = []\n",
        "\n",
        "  # Performing Aspect Extraction\n",
        "  for sen in mixed_sen:\n",
        "    important = sp(sen)\n",
        "    descriptive_item = ''\n",
        "    target = ''\n",
        "    for token in important:\n",
        "      if token.dep_ == 'nsubj' and token.pos_ == 'NOUN':\n",
        "        target = token.text\n",
        "      if token.pos_ == 'ADJ':\n",
        "        added_terms = ''\n",
        "        for mini_token in token.children:\n",
        "          if mini_token.pos_ != 'ADV':\n",
        "            continue\n",
        "          added_terms += mini_token.text + ' '\n",
        "        descriptive_item = added_terms + token.text \n",
        "    ext_aspects.append({'aspect': target,'description': descriptive_item})\n",
        "\n",
        "  print(\"ASPECT EXTRACTION\\n\")\n",
        "  print(ext_aspects)\n",
        "\n",
        "\n",
        "  for aspect in ext_aspects:\n",
        "    aspect['sentiment'] = TextBlob(aspect['description']).sentiment\n",
        "\n",
        "  print(\"\\n\")\n",
        "  print(\"\\nSENTIMENT ASSOCIATION\\n\")\n",
        "  print(ext_aspects)\n"
      ],
      "metadata": {
        "id": "K4dIm11yfpM4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model"
      ],
      "metadata": {
        "id": "oQYe9laFg76H"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading\n",
        "with open('tokenizer_sentiment.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)"
      ],
      "metadata": {
        "id": "YCaHhAhojgZL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load model\n",
        "\n",
        "def predict_sentiment(text):\n",
        "    '''Function to predict sentiment class of the passed text'''\n",
        "    model = load_model('timesofindia_lstm_model_for_sentiment.h5')\n",
        "\n",
        "    sentiment_classes = ['Negative', 'Positive']\n",
        "    max_len=50\n",
        "    \n",
        "    # Transforms text to a sequence of integers using a tokenizer object\n",
        "    xt = tokenizer.texts_to_sequences(text)\n",
        "    # Pad sequences to the same length\n",
        "    xt = pad_sequences(xt, padding='post', maxlen=max_len)\n",
        "    # Do the prediction using the loaded model\n",
        "    yt = model.predict(xt).argmax(axis=1)\n",
        "    # Print the predicted sentiment\n",
        "    print('The predicted sentiment is', sentiment_classes[yt[0]])\n",
        "\n",
        "    \n",
        "\n",
        "    \n",
        "\n",
        "   "
      ],
      "metadata": {
        "id": "k0NWr8J9fpPp"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "# Load model\n",
        "model = load_model('timesofindia_lstm_model_for_topic.h5')\n",
        "\n",
        "def predict_topic(text):\n",
        "    '''Function to predict Topic class of the passed text'''\n",
        "    model = load_model('timesofindia_lstm_model_for_topic.h5')\n",
        "\n",
        "    topic_classes = ['Topic 1', 'Topic 2', 'Topic 3', 'Topic 4', 'Topic 5', 'Topic 6', 'Topic 7', 'Topic 8', 'Topic 9', 'Topic 10', 'Topic 11', 'Topic 12','Topic 13', 'Topic 14', 'Topic 15', 'Topic 16', 'Topic 17', 'Topic 18', 'Topic 19', 'Topic 20', 'Topic 21', 'Topic 22', 'Topic 23', 'Topic 24']\n",
        "    max_len=50\n",
        "    \n",
        "    # Transforms text to a sequence of integers using a tokenizer object\n",
        "    xt = tokenizer.texts_to_sequences(text)\n",
        "    # Pad sequences to the same length\n",
        "    xt = pad_sequences(xt, padding='post', maxlen=max_len)\n",
        "    # Do the prediction using the loaded model\n",
        "    yt = model.predict(xt).argmax(axis=1)\n",
        "    # Print the predicted sentiment\n",
        "    print('\\nThe predicted Topic is', topic_classes[yt[0]])\n",
        "\n",
        "    \n",
        "\n",
        "    #for apspect based sentiment prediction\n",
        "    # converting list to string type\n",
        "    str1 = \" \" \n",
        "    \n",
        "    # return string  \n",
        "    txt= str1.join(text)\n",
        "    predict_aspect(txt)\n",
        "\n",
        "   "
      ],
      "metadata": {
        "id": "gHUZqNjTqMw7"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sent_topic(text):\n",
        "  get_txt=text\n",
        "\n",
        "  predict_sentiment(get_txt)\n",
        "  predict_topic(get_txt)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SCHj6gKYqlPL"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_sent_topic(['the battery is performing good'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sh7hPYgfpSP",
        "outputId": "e5769823-33ba-44d9-b311-43f84e42a2e3"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The predicted sentiment is Positive\n",
            "\n",
            "The predicted Topic is Topic 21\n",
            "ASPECT EXTRACTION\n",
            "\n",
            "[{'aspect': 'battery', 'description': 'good'}]\n",
            "\n",
            "\n",
            "\n",
            "SENTIMENT ASSOCIATION\n",
            "\n",
            "[{'aspect': 'battery', 'description': 'good', 'sentiment': Sentiment(polarity=0.7, subjectivity=0.6000000000000001)}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_LlhE0lfsQeF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}